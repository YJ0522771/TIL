# AI math



## 벡터

### norm

* 원점에서부터의 거리.

#### L1

* 변화량의 절대값의 합.
* ||(x, y)||1 = x + y

#### L2

* 유클리드 거리
* ||(x, y)||2 = (x^2 + y^2)^1/2



## 행렬





## 경사하강법



## 딥러닝 학습 방법

* **소프트맥스(softmax) 연산** : 모델의 출력을 확률로 해석할 수 있게 변환.
* 소프트맥스 연산을 사용해서 *확률 벡터로 변환*한 다음 **학습**.
* **추론**을 하는 경우는 소프트맥스가 굳이 필요하지 않다.



### 활성함수 (activation function)

* 비선형함수.
* 각 노드에 개별적으로 적용.
* 모든 출력값을 고려하는 소프트맥스와 차이.
* 선형모델로 나온 출력값에 활성함수를 씌움.
* 활성함수를 사용하지 않으면 선형모형과 차이가 없음.
* 



### MLP

* 이론적으로는 2층 신경망으로도 임의의 연속함수를 근사 가능.
* 층이 깊을수록 목적함수를 근사하는데 필요한 노드의 수가 훨씬 빨리 줄어든다.
  * 적은 파라미터로 복잡한 함수 표현 가능.
  * 반드시 최적화가 쉽다고 할 수는 없다.



### backpropagation

* 각 층의 gradient vector는 윗층부터 역순으로 계산.
* 각 노드의 미분 값을 모두 저장해야하므로, forward propagation보다 메모리를 많이 소비한다.



## 베이즈 통계학

* 새로운 데이터가 들어오면, 앞서 계산한 사후확률을 사전확률로 사용하여 갱신된 사후확률을 구할 수 있다.
* 조건부 확률을 인과관계 추론에 함부로 사용하면 안 됨.
* 중첩요인 효과를 제거해야함. 
* 가짜 연관성 (spurious cerrelation)



## CNN

* fully connected 구조 : 가중치를 저장하는 공간이 커짐. 학습량이 많음.



* 커널에 따라 이미지가 다양하게 변화.
* 데이터의 종류에 따라 다양한 차원의 convolution 계산 가능.



## RNN

* 시퀀스 데이터를 다루기 위해서는 가변적인 데이터를 다룰 수 있는 모델 필요.
  * 조건부의 데이터가 가변적.
* 자기회귀 모델 (Autoregressive Model)
  * 고정된 길이 τ만큼의 시퀀스만 사용.
  * `AR(τ)`
* 잠재 자기회귀 모델
  * 바로 직전 정보를 제외한 나머지 정보들을 `Ht`라는 잠재변수로 인코딩하여 활용.



* backpropagation through time (BPTT)
  * 시퀀스가 길어지면 gradient가 불안정해짐.
  * truncated BPTT : gradient 전달을 일정 길이에서 끊음.