# 03/11

### 할 일

* 6-2강 AutoGrad
* 심화과제 1 CNN Visualization



### 피어세션

* 



### 공부한 내용

#### AutoGrad

* Aurograd : Automatic gradient calculating API
* DL library - forward & backward passes



```python
y = x * 3
y.backward(g) # 인자가 들어가면, backward로 계산된 gradient 값에 g를 곱한 값이 출력.
x.grad # y의 x에 대한 gradient
# default로 chain rule에 의해 이전 층이 gradient가 들어감.
```

* x를 선언할 때, `requires_grad=True`로 해줘야 `x.grad`를 할 수 있다.
* 같은 층의 `.backward`를 여러 번 호출하면 중첩 됨. (더해짐.)
  * default는 다른 backward가 호출 되면, 이전 backward의 결과는 사라지게 됨.
  * 메모리 문제. 일반적으로는 마지막 gradient 값만 필요하다.
  * `retain_graph=True`로 하면 다음 backward가 호출되어도 사라지지 않음.
* backward hook에서 gradient 결과를 바꾸고 싶으면 return.
* `register_hook`의 반환값은 핸들러 (h).
  * `h.remove()`로 해당 hook을 지울 수 있다.