# 02/10

### 할 일

* DL Basic
  * 심화과제 - Vision Transformer




### 피어세션



### 공부한 내용

#### Vision Transformer

> [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
>
> [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

 

* Transformer의 구조.
  * 여러 개의 encoder block.
    * 여러 레이어의 Multi-head self-attention.
    * MLP block.
  * attention 출력값 (representation)은 모든 encoder에서 나오는 값들을 저장해두어야한다. 